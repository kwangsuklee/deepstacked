{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dataset.shape ==  (245760, 1)\n",
      " dataset2.shape ==  (245760, 1)\n",
      " dataset.shape ==  (245760, 1)\n",
      " dataset2.shape ==  (245760, 1)\n",
      "mean of train ==  0.45230213\n",
      "     mean type of train --  <class 'numpy.float32'>\n",
      "std of train ==  0.032692585\n",
      "mean of test ==  0.5687847\n",
      "     mean type of test --  <class 'numpy.float32'>\n",
      "std of test ==  0.033779364\n",
      "length of train ==  245760\n",
      "length of test ==  245760\n",
      "length of Total ==  491520\n",
      " Lets start ~!!! go, go! \n",
      "-----------------------------------\n",
      " train_X.shape =  (245727, 32, 1)\n",
      " train_Y.shape =  (245727,)\n",
      " test_X.shape ==  (245727, 32, 1)\n",
      " test_Y.shape ==  (245727,)\n",
      "-----------------------------------\n",
      "Train on 245727 samples, validate on 245727 samples\n",
      "Epoch 1/1\n",
      "245727/245727 [==============================] - 544s 2ms/step - loss: 0.0012 - mean_squared_error: 0.0012 - mean_absolute_error: 0.0254 - mean_absolute_percentage_error: 1857.8771 - val_loss: 0.0158 - val_mean_squared_error: 0.0158 - val_mean_absolute_error: 0.1214 - val_mean_absolute_percentage_error: 1842.7024\n",
      "Train on 245727 samples, validate on 245727 samples\n",
      "Epoch 1/1\n",
      "245727/245727 [==============================] - 540s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1858.4475 - val_loss: 0.0217 - val_mean_squared_error: 0.0217 - val_mean_absolute_error: 0.1434 - val_mean_absolute_percentage_error: 1755.7110\n",
      "Train on 245727 samples, validate on 245727 samples\n",
      "Epoch 1/1\n",
      "245727/245727 [==============================] - 537s 2ms/step - loss: 0.0011 - mean_squared_error: 0.0011 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 1864.3840 - val_loss: 0.0233 - val_mean_squared_error: 0.0233 - val_mean_absolute_error: 0.1489 - val_mean_absolute_percentage_error: 1730.8391\n",
      "Train on 245727 samples, validate on 245727 samples\n",
      "Epoch 1/1\n",
      "245727/245727 [==============================] - 541s 2ms/step - loss: 0.0010 - mean_squared_error: 0.0010 - mean_absolute_error: 0.0242 - mean_absolute_percentage_error: 1845.9138 - val_loss: 0.0215 - val_mean_squared_error: 0.0215 - val_mean_absolute_error: 0.1428 - val_mean_absolute_percentage_error: 1726.2416\n",
      "Train on 245727 samples, validate on 245727 samples\n",
      "Epoch 1/1\n",
      "245727/245727 [==============================] - 538s 2ms/step - loss: 9.5609e-04 - mean_squared_error: 9.5609e-04 - mean_absolute_error: 0.0232 - mean_absolute_percentage_error: 1791.8456 - val_loss: 0.0201 - val_mean_squared_error: 0.0201 - val_mean_absolute_error: 0.1382 - val_mean_absolute_percentage_error: 1696.0913\n",
      "Train on 245727 samples, validate on 245727 samples\n",
      "Epoch 1/1\n",
      "245727/245727 [==============================] - 539s 2ms/step - loss: 8.8228e-04 - mean_squared_error: 8.8228e-04 - mean_absolute_error: 0.0222 - mean_absolute_percentage_error: 1755.1325 - val_loss: 0.0211 - val_mean_squared_error: 0.0211 - val_mean_absolute_error: 0.1422 - val_mean_absolute_percentage_error: 1646.7904\n",
      "Train on 245727 samples, validate on 245727 samples\n",
      "Epoch 1/1\n",
      "245727/245727 [==============================] - 538s 2ms/step - loss: 8.6432e-04 - mean_squared_error: 8.6432e-04 - mean_absolute_error: 0.0220 - mean_absolute_percentage_error: 1755.1172 - val_loss: 0.0206 - val_mean_squared_error: 0.0206 - val_mean_absolute_error: 0.1405 - val_mean_absolute_percentage_error: 1650.3449\n",
      "Train on 245727 samples, validate on 245727 samples\n",
      "Epoch 1/1\n",
      "245727/245727 [==============================] - 537s 2ms/step - loss: 8.5955e-04 - mean_squared_error: 8.5955e-04 - mean_absolute_error: 0.0219 - mean_absolute_percentage_error: 1754.3983 - val_loss: 0.0194 - val_mean_squared_error: 0.0194 - val_mean_absolute_error: 0.1362 - val_mean_absolute_percentage_error: 1666.4867\n",
      "Train on 245727 samples, validate on 245727 samples\n",
      "Epoch 1/1\n",
      "245727/245727 [==============================] - 537s 2ms/step - loss: 8.5557e-04 - mean_squared_error: 8.5557e-04 - mean_absolute_error: 0.0218 - mean_absolute_percentage_error: 1755.8689 - val_loss: 0.0182 - val_mean_squared_error: 0.0182 - val_mean_absolute_error: 0.1317 - val_mean_absolute_percentage_error: 1682.9338\n",
      "Train on 245727 samples, validate on 245727 samples\n",
      "Epoch 1/1\n",
      "245727/245727 [==============================] - 529s 2ms/step - loss: 8.5103e-04 - mean_squared_error: 8.5103e-04 - mean_absolute_error: 0.0217 - mean_absolute_percentage_error: 1756.2641 - val_loss: 0.0173 - val_mean_squared_error: 0.0173 - val_mean_absolute_error: 0.1279 - val_mean_absolute_percentage_error: 1697.0306\n",
      " Start Prediction ... \n",
      " Train X_1 (bearing 3 x)  Predicting...\n",
      "245727/245727 [==============================] - 153s 624us/step\n",
      " Test X_1 (bearing 3 x) Predicting...\n",
      "245727/245727 [==============================] - 153s 623us/step\n",
      "Train Score: 0.20000 RMSE\n",
      "Test Score: 0.90531 RMSE\n",
      "--------------------------------------------------------\n",
      " history.losses      =  [0.0011981431402308413, 0.0010692596637084805, 0.0010613927030588182, 0.0010358390688878843, 0.0009560866000507142, 0.000882278548641256, 0.0008643183442910906, 0.0008595450341827125, 0.0008555653857770073, 0.0008510286105837999]\n",
      " history.mses(=loss) =  [0.0011981431402308413, 0.0010692596637084805, 0.0010613927030588182, 0.0010358390688878843, 0.0009560866000507142, 0.000882278548641256, 0.0008643183442910906, 0.0008595450341827125, 0.0008555653857770073, 0.0008510286105837999]\n",
      " history.maes        =  [0.025429726287494517, 0.02453014174725357, 0.02445504381255632, 0.024162337327891616, 0.023184856723421732, 0.02220344843980034, 0.02195136649778907, 0.021879993287828428, 0.021817058092141434, 0.021746747595868702]\n",
      " history.mapes       =  [1857.8770623434987, 1858.4474620219046, 1864.3839628027886, 1845.9138109493695, 1791.8455983409413, 1755.132470614596, 1755.117159512253, 1754.3983145789052, 1755.8688563870035, 1756.264085261021]\n",
      " history.val_losses     =  [0.015841686715803206, 0.021670217418301822, 0.023275478519232028, 0.021451272797623794, 0.02005459847728883, 0.02113327083707038, 0.020630876438294837, 0.01944414328430228, 0.018224193774688542, 0.017254382151168778]\n",
      " history.val_mses(=loss)=  [0.015841686715803206, 0.021670217418301822, 0.023275478519232028, 0.021451272797623794, 0.02005459847728883, 0.02113327083707038, 0.020630876438294837, 0.01944414328430228, 0.018224193774688542, 0.017254382151168778]\n",
      " history.val_maes       =  [0.12144868005728296, 0.14340614012187197, 0.1489045414215334, 0.14283291736545317, 0.13817734696136594, 0.14220469900723356, 0.14045874009698694, 0.1361927070682979, 0.13166044896184742, 0.12794204967577763]\n",
      " history.val_mapes      =  [1842.7023557902587, 1755.710998457163, 1730.8390888858332, 1726.2416056598481, 1696.0912811601484, 1646.7904290884503, 1650.3449042002003, 1666.4867339989344, 1682.9338100119544, 1697.030567532586]\n",
      "--------------------------------------------------------\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 1)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 32, 64)       128         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 64)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 32, 32)       2080        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 32)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32)       0           input_1[0][0]                    \n",
      "                                                                 dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 32, 16)       2624        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 32, 32)       4224        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32)       0           add_1[0][0]                      \n",
      "                                                                 bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 32, 32)       8320        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 32)           8320        lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 8)            264         lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            9           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 25,969\n",
      "Trainable params: 25,969\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "model summary ...  None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      " history.losses(mse)     ==  [0.0011981431402308413, 0.0010692596637084805, 0.0010613927030588182, 0.0010358390688878843, 0.0009560866000507142, 0.000882278548641256, 0.0008643183442910906, 0.0008595450341827125, 0.0008555653857770073, 0.0008510286105837999]\n",
      " history.val_losses(mse) ==  [0.015841686715803206, 0.021670217418301822, 0.023275478519232028, 0.021451272797623794, 0.02005459847728883, 0.02113327083707038, 0.020630876438294837, 0.01944414328430228, 0.018224193774688542, 0.017254382151168778]\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "----------------------- The end of code ----------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nplt.plot(history.losses, 'bo')  # Train losses = MSE \\nplt.plot(history.val_losses, 'b') # Test losses = MSE \\n##### 아래 주의, x축은 epochs ### \\nplt.axis([0, epochs, 0.00, 0.04]) ### y-end의 val_loss가 0.22를 넘지 않음.\\n##### x-start(0), x-end(에폭 수), y-start(0), y-and #####\\nplt.title('Model Loss, MAE of Bearing3 X-axis from 14:17 to 16:07')\\nplt.ylabel('Loss(Mean Squared Error)')\\nplt.xlabel('Epoch')\\n#plt.legend(['Train', 'Test'], loc=0)  #plt.plot(,,,label=)\\nplt.legend(['Train', 'Test'])\\nplt.savefig(title+'_MAE_Model_Loss.png')\\n#plt.figure() ## \\nplt.show() \\nprint('--------------------------------------------------------')  \\nplt.plot(history.mapes, 'bo') # Train MAPE\\nplt.plot(history.val_mapes, 'b') # Test MAPE\\n##### 아래 주의, x축은 epochs ### \\nplt.axis([0, epochs, 0.00, 60000]) ### y-end의 val_loss가 30000를 넘지 않음.\\n##### x-start(0), x-end(에폭 수), y-start(0), y-and #####\\nplt.title('Prediction Loss(MAPE) of Bearing3 X-axis from 14:17 to 16:07')\\nplt.ylabel('Loss(Mean Absolute Percentage Error)')\\nplt.xlabel('Epoch')\\n#plt.legend(['Train', 'Test'], loc=0)  #plt.plot(,,,label=)\\nplt.legend(['Train', 'Test'])\\nplt.savefig(title+'_MAPE_Predict_Loss.png')\\n#plt.figure() ## \\nplt.show() \\nprint('--------------------------------------------------------')  \\nplt.plot(history.mses, label='MSE of Train') # Train 0.000x소수점단위\\nplt.plot(history.maes, label='MAE of Train') # Train 0.000x소수점단위\\n#plt.plot(history.mapes, label='mean_absolute_percentage_erro') # 1000~2000단위, 출력만 제외 \\n##### 아래 주의, x축은 epochs ### \\nplt.axis([0, epochs, 0.000, 0.040]) ### y-end의 val_loss가 0.025를 넘지 않음.\\n##### x-start(0), x-end(에폭 수), y-start(0), y-and #####\\nplt.title('Train Loss of Bearing3 X-axis from 12:17 to 14:07') \\nplt.ylabel('Train Loss')\\nplt.xlabel('Train Epoch')\\nplt.legend() ## plt.plot(,label='')\\nplt.savefig(title+'_Train_Loss.png') ###### 수정 \\nplt.show()\\nprint('--------------------------------------------------------') \\nplt.plot(history.val_mses, label='Test_MSE') # Test 0.000x 소수점단위\\nplt.plot(history.val_maes, label='Test_MAE') # Test 0.000x 소수점단위 \\n#plt.plot(history.val_mapes, label='validation_MAPE') # 1000~2000단위, 출력만 제외 \\n##### 아래 주의, x축은 epochs ### \\nplt.axis([0, epochs, 0.00, 0.28]) ### y-end의 val_loss가 0.14를 넘지 않음.\\n##### x-start(0), x-end(에폭 수), y-start(0), y-and #####\\nplt.title('Prediction Test Loss of Bearing3 X-axis from 14:17 to 16:07') \\nplt.ylabel('Test Loss')\\nplt.xlabel('Test Epoch')\\nplt.legend() ## plt.plot(,label='')\\nplt.savefig(title+'_Test_Loss.png') ###### 수정 \\nplt.show()\\nprint('--------------------------------------------------------') \\n# ---------------------------------\\nseries1 = dataframe #dataframe = pandas.read_csv(t, header=None, sep='\\t', usecols=[4], engine='python')\\nseries1.plot(kind='kde') # ‘kde’ : Kernel Density Estimation plot, ‘line’ : line plot (default) \\nplt.title('Density Plots of Train Dataset(Bearing3 X-axis from 12:17 to 14:07)') \\nplt.savefig(title+'_dataset_DensityPlots.png') \\nplt.show()\\n# ---------------------------------\\nseries2 = dataframe2 #dataframe2 = pandas.read_csv(t1, header=None, sep='\\t', usecols=[4], engine='python')\\nseries2.plot(kind='kde') # ‘kde’ : Kernel Density Estimation plot, ‘line’ : line plot (default) \\nplt.title('Density Plots of Test Dataset(Bearing3 X-axis from 14:17 to 16:07)') \\nplt.savefig(title+'_dataset2_DensityPlots.png') \\nplt.show()\\n# ---------------------------------\\n# box and whisker plot\\n# from pandas import DataFrame\\ndf_dataset = pandas.DataFrame(dataset) #Train dataset\\ndf_dataset.boxplot() \\nplt.savefig(title+'_boxplot_dataset.png') \\nplt.show() \\nprint('------- df_dataset.describe() -------')\\nprint(df_dataset.describe())\\n# -------------------------------------\\ndf_trainPredictPlot = pandas.DataFrame(trainPredictPlot)\\ndf_trainPredictPlot.boxplot()  \\nplt.savefig(title+'_boxplot_trainpredict.png') \\nplt.show() \\nprint('------- df_trainPredictPlot.describe() -------')\\nprint(df_trainPredictPlot.describe())\\n# -------------------------------------\\n# box and whisker plot\\n# from pandas import DataFrame\\ndf_dataset2 = pandas.DataFrame(dataset2) #Test dataset\\ndf_dataset2.boxplot() \\nplt.savefig(title+'_boxplot_dataset2.png') \\nplt.show() \\nprint('------- df_dataset2.describe() -------')\\nprint(df_dataset2.describe())\\n# -------------------------------------\\ndf_testPredictPlot = pandas.DataFrame(testPredictPlot)\\ndf_testPredictPlot.boxplot()  # aa.boxplot()에서 aa는 dataframe 타입 \\nplt.savefig(title+'_boxplot_test_predict.png') \\nplt.show() \\nprint('------- df_testPredictPlot.describe() -------')\\nprint(df_testPredictPlot.describe())\\n# -----------------------------------------------------\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "C(64)–C(32)-BL(8)-BL(16)-L(32)-L(32)\n",
    "with residualnet only. \n",
    "\n",
    "Train Score: 0.20000 RMSE\n",
    "Test Score: 0.90531 RMSE\n",
    "--------------------------------------------------------\n",
    " history.losses      =  0.0008510286105837999\n",
    " history.mses(=loss) =  0.0008510286105837999\n",
    " history.maes        =  0.021746747595868702\n",
    " history.mapes       =  1756.264085261021\n",
    "\n",
    " history.val_losses     =  0.017254382151168778\n",
    " history.val_mses(=loss)=  0.017254382151168778\n",
    " history.val_maes       =  0.12794204967577763\n",
    " history.val_mapes      =  1697.030567532586\n",
    " \n",
    "\"\"\"\n",
    "# Version. \n",
    "# 12.15. conv.2layers \n",
    "# 12.16. conv.4layers  \n",
    "# \n",
    "# 유의사항: 절대하지말것. conv1d에서는 BN, Pooling적용 시, 언더피팅, 퍼포먼스 감소. \n",
    "# 삭제 visible1 = Input(shape=(204770, 29))  #### 아래 strides를 1로 해야 acc향상 \n",
    "# 추가 visible1 = Input(shape=(timesteps, features))  # 29, 1\n",
    "# 성공 \n",
    "# cnn에서 stride를 1로 해야 accuracy 향상, 속도 느려짐. \n",
    "\"\"\"  \n",
    "2018.12.11.화.23시10분. success  \n",
    "2018.12.12.수.0시. sucess. shortcut(skip) connection, residual net. \n",
    "\n",
    "\"\"\"\n",
    "#  Timestep = 29,  (19는 Loss크다)\n",
    "#  A Type code : User defined History Class \n",
    "#       class LossHistory(keras.callbacks.Callback):  #history = LossHistory()\n",
    "#             def ... def ... \n",
    "#        model.fit( )  # # because of if loop for fit() \n",
    "#  B Type code : Default typed History Class \n",
    "#       compile \n",
    "#       history = model.fit( )\n",
    "#       score = model.evaluate \n",
    "# Kwangsuk.Lee, 2018.6.12.\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas     \n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame \n",
    "from pandas import Series \n",
    "import math\n",
    "import keras      \n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dense, Input, Flatten, Add, concatenate \n",
    "from keras.layers import Activation, BatchNormalization, regularizers, Dropout\n",
    "\n",
    "#from keras.layers.noise import GaussianNoise\n",
    "#from keras.layers import Conv1D, MaxPooling1D  \n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "# keras.layers.pooling.GlobalMaxPooling1D() no input parameters.\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "from keras.layers import LSTM, Bidirectional \n",
    "from keras.utils import plot_model  # New \n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error #Mean squared error regression loss\n",
    "from sklearn.metrics import mean_absolute_error # Mean absolute error regression loss\n",
    "# http://scikit-learn.org/stable/modules/classes.html \n",
    "#http://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n",
    "# http://scikit-learn.org/stable/modules/classes.html\n",
    "from sklearn.metrics import recall_score, precision_score # New \n",
    "from sklearn.metrics import f1_score #f1_score(y_true, y_pred[, labels, …])\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn import preprocessing  # New \n",
    "#from keras.layers import normalization ### 추가 keras.layers.normalization.BatchNormalization() \n",
    "#from keras.layers import Dropout  ### 추가 keras.layers.Dropout()\n",
    "\n",
    "\n",
    "title = '190320_08_resnet_only_' \n",
    "epochs = 10 \n",
    "batch_size = 128 # 64, 128, do not 256, i.e. in case of 128, 1 epoch = 9minutes     \n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):   #history = LossHistory()\n",
    "    def init(self):         # history.init() \n",
    "        self.losses = []\n",
    "        #self.accs = []\n",
    "        self.val_losses = []\n",
    "        #self.val_accs = []\n",
    "        self.mses = []\n",
    "        self.maes = []\n",
    "        self.mapes = [] \n",
    "        self.val_mses = []\n",
    "        self.val_maes = []\n",
    "        self.val_mapes = []   \n",
    "    \n",
    "    def on_epoch_end(self, batch, logs={}): \n",
    "        self.losses.append(logs.get('loss'))\n",
    "        #self.accs.append(logs.get('acc'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        #self.val_accs.append(logs.get('val_accuracy')) \n",
    "        self.mses.append(logs.get('mean_squared_error')) \n",
    "        self.maes.append(logs.get('mean_absolute_error')) \n",
    "        self.mapes.append(logs.get('mean_absolute_percentage_error')) \n",
    "        self.val_mses.append(logs.get('val_mean_squared_error')) \n",
    "        self.val_maes.append(logs.get('val_mean_absolute_error')) \n",
    "        self.val_mapes.append(logs.get('val_mean_absolute_percentage_error'))       \n",
    "# end of the class \n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, timesteps): #기존def create_dataset(dataset, timesteps=1): 여기서 -1삭제\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-timesteps-1):\n",
    "        a = dataset[i:(i+timesteps), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + timesteps, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# fix random seed for reproducibility \n",
    "numpy.random.seed() \n",
    "\n",
    "# load the dataset\n",
    "t = '00_Train_2003.11.25.12.17_14.07_12sets.csv' # 12sets \n",
    "dataframe = pandas.read_csv(t, header=None, sep='\t', usecols=[4], engine='python')  \n",
    "dataset = dataframe.values  # 여기는 항상 t 시점\n",
    "dataset = dataset.astype('float32')\n",
    "print(' dataset.shape == ', dataset.shape)\n",
    "\n",
    "t1 = '00_Test_2003.11.25.14.17_16.07_12sets.csv' # 12sets \n",
    "dataframe2 = pandas.read_csv(t1, header=None, sep='\t', usecols=[4], engine='python')\n",
    "dataset2 = dataframe2.values  # Prediction Target(label) Data\n",
    "dataset2 = dataset2.astype('float32')\n",
    "print(' dataset2.shape == ', dataset2.shape)\n",
    "\n",
    "# normalize the dataset ---------------- # 18.2.25. \n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "scaler2 = MinMaxScaler(feature_range=(0, 1)) # New 18.3.16.금.14시34분\n",
    "dataset2 = scaler2.fit_transform(dataset2)\n",
    "\n",
    "print(' dataset.shape == ', dataset.shape)\n",
    "print(' dataset2.shape == ', dataset2.shape)\n",
    "\n",
    "## normalize the dataset ---------------# New 18.5.21.월.15시30분\n",
    "mean1 = numpy.mean(dataset)  \n",
    "print('mean of train == ', mean1)\n",
    "print('     mean type of train -- ' , type(numpy.mean(dataset)) )\n",
    "# dataset -= mean1 ### 차분, 노멀라이즈 \n",
    "## 분산 std1 = sum((dataset-numpy.mean(dataset)**2))/int(len(dataset))  \n",
    "std1 = numpy.std(dataset) # 표준편차 std \n",
    "print('std of train == ', std1)\n",
    "# dataset /= std1 ### 차분, 노멀라이즈 \n",
    "## normalize the dataset ---------------# New 18.5.21.월.15시30분\n",
    "mean2 = numpy.mean(dataset2)  \n",
    "print('mean of test == ', mean2)\n",
    "print('     mean type of test -- ' , type(numpy.mean(dataset2)) )\n",
    "# dataset2 -= mean2 ### 차분, 노멀라이즈 \n",
    "## 분산 std1 = sum((dataset-numpy.mean(dataset)**2))/int(len(dataset))  \n",
    "std2 = numpy.std(dataset2) # 표준편차 std \n",
    "print('std of test == ', std2)\n",
    "# dataset2 /= std2 ### 차분, 노멀라이즈\n",
    "\n",
    "# split into train and test sets ----------------\n",
    "train_size = int(len(dataset)) # train 102400/5(batch_size)=  ,102396 ->102380   \n",
    "test_size = int(len(dataset2)) # test 20480/5(batch_size)=   ,20478\n",
    "train = dataset[0:train_size,:] \n",
    "test = dataset2[0:test_size,:]\n",
    "print('length of train == ', int(len(dataset)))\n",
    "print('length of test == ', int(len(dataset2))) \n",
    "print('length of Total == ', int(len(dataset))+int(len(dataset2))) \n",
    "print(' Lets start ~!!! go, go! ') \n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "features = 1\n",
    "timesteps = 32 # 29 # batch_size(3) X 3, 100설정 시, epoch 당 8시간 소요, timesteps=lookback은 한 스텝을 예측 \n",
    "\n",
    "trainX, trainY = create_dataset(train, timesteps)\n",
    "testX, testY = create_dataset(test, timesteps)\n",
    "# reshape input to be [samples, time steps, features] 타임스텝이란 하나의 샘플에 포함된 시퀀스 갯수\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "print('-----------------------------------')  \n",
    "print(' train_X.shape = ', trainX.shape) # train_X.shape = (204770, 29, 1)\n",
    "print(' train_Y.shape = ', trainY.shape) # train_Y.shape = (204770,)\n",
    "print(' test_X.shape == ', testX.shape)  # test_X.shape == (40930, 29, 1) \n",
    "print(' test_Y.shape == ', testY.shape)  # test_Y.shape == (40930,)\n",
    "print('-----------------------------------')  \n",
    "\n",
    "# create and fit the CNN and LSTM network\n",
    "# 2018.12.9.일.23시.\n",
    "# visible1 = Input(shape=(204770, 29))  #### 아래 strides를 1로 해야 acc향상 \n",
    "# visible1 = Input(shape=(timesteps, features))  # 29, 1\n",
    "# filter = output, must same, kernel = input \n",
    "#conv11 = Conv1D(filters=32, kernel_size=1, strides=1, activation='relu')(visible1)\n",
    "####### BN 하지 않는다. Pooling 하지 않는다. ####### \n",
    "####### 1D에서는 pooling을 하면 성능 악화. 1D에서는 BN도 성능 악화.  \n",
    "\n",
    "\n",
    "# C(32)-C(32)-BL(4)-BL(8)-L(16)-L(16)\n",
    "# Dropout 0.5 \n",
    "\n",
    "######################################### with K.tf.device('/gpu:0'):  => GPU usage\n",
    "import keras.backend.tensorflow_backend as K \n",
    "with K.tf.device('/gpu:0'):\n",
    "    visible1 = Input(shape=(timesteps, features))  # 32, 1 # 29, 1\n",
    "     \n",
    "    conv_11 = Conv1D(filters=64, kernel_size=1, strides=1, activation='relu')(visible1)\n",
    "    conv_11_out = Dropout(0)(conv_11)\n",
    "    conv_12 = Conv1D(filters=32, kernel_size=1, strides=1, activation='relu')(conv_11_out)\n",
    "    conv_12_out = Dropout(0)(conv_12)\n",
    "    \n",
    "    residual_1 = keras.layers.Add()([visible1, conv_12_out]) # 32, 32\n",
    "    \n",
    "    bi11 = (Bidirectional(LSTM(8, return_sequences=True, dropout=0)))(residual_1) ### success \n",
    "    bi12 = (Bidirectional(LSTM(16, return_sequences=True, dropout=0)))(bi11)  \n",
    "    #bi11 = (Bidirectional(LSTM(16, return_sequences=True, kernel_regularizer=l2(0.001))))(residual_1) ### success \n",
    "    #bi12 = (Bidirectional(LSTM(16, return_sequences=True, kernel_regularizer=l2(0.001))))(bi11)  \n",
    "    \n",
    "    residual_2 = keras.layers.Add()([residual_1, bi12]) # 32, 32 (16x2)\n",
    "    ls11 = (LSTM(32, return_sequences=True, dropout=0))(residual_2) # 32\n",
    "    ls12 = (LSTM(32, dropout=0))(ls11) # do not use retrun_sequence=True at last lstm layer \n",
    "    #ls11 = (LSTM(32, return_sequences=True, kernel_regularizer=l2(0.001)))(residual_2) # 32\n",
    "    #ls12 = (LSTM(16, kernel_regularizer=l2(0.001)))(ls11) # do not use retrun_sequence=True at last lstm layer \n",
    "    \n",
    "    ##########  (29, 16) (12,) ,  (29, 16) (29,)\n",
    "    # res12 = keras.layers.Add()([res11, ls12]) #########\n",
    "    dense11 = Dense(8, activation='relu')(ls12)\n",
    "    out1 = Dense(1, activation='relu')(dense11)\n",
    "    model = Model(inputs=[visible1], outputs=[out1]) # multi-input, multi-output \n",
    "    \n",
    "    # from keras.regularizers import l2\n",
    "    # keras.regularizers.l1(0.)\n",
    "    # keras.regularizers.l2(0.)\n",
    "    # keras.regularizers.l1_l2(l1=0.01, l2=0.01)\n",
    "\n",
    "    # = (LSTM(32, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01))\n",
    "    # = Conv2D(32, (3,3), kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "    # = Dense(32, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))\n",
    "    # = Conv1D(filters=128, kernel_size=1, strides=1, kernel_regularizer=regularizers.l2(0.001))(conv_31)             \n",
    "    #########################################\n",
    "    optimizer = keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, amsgrad=False) \n",
    "\n",
    "    #checkpoint = ModelCheckpoint(saveweightfile, monitor='loss', verbose=1, save_weights_only=True, save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "    reducelr = ReduceLROnPlateau(monitor='val_loss', factor = 0.1, patience=1) #에포크 1동안 좋아지지 않으면 호출 \n",
    "    \n",
    "    csv_logger = CSVLogger(title + 'log.csv', append=True, separator='\\n')  # separator=';', separator='\\t'\n",
    "        \n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizer,\n",
    "                  #optimizer='adam',\n",
    "                  metrics=['mse', 'mae', 'mape'])\n",
    "    \n",
    "    batch_size = batch_size  #  avove gloval varibale \n",
    "    epochs=epochs   # avove gloval varibale  \n",
    "    \n",
    "    history = LossHistory()#callbacks=[history] above model.fit\n",
    "    history.init()         #callbacks=[history] above model.fit\n",
    "    \n",
    "    for i in range(epochs):   ### 10 or 20, 아래 epochs가 1,일, 따라서 1 X 수정10 = 10회 epochs\n",
    "        model.fit(trainX, trainY, \n",
    "                  epochs=1, \n",
    "                  batch_size=batch_size, \n",
    "                  shuffle=False,   \n",
    "                  validation_data=(testX, testY),\n",
    "                  callbacks=[history, reducelr, csv_logger])  # , checkpoint  \n",
    "        model.reset_states() # shuffle=False 순서대로 출력의미, LSTM레이어가 여럿일때는 fit(return_sequence=True) \n",
    "#################################################\n",
    "# make predictions\n",
    "print(' Start Prediction ... ')   \n",
    "print(' Train X_1 (bearing 3 x)  Predicting...') \n",
    "trainPredict = model.predict(trainX, batch_size=batch_size, verbose = 1)\n",
    "model.reset_states()\n",
    "print(' Test X_1 (bearing 3 x) Predicting...') \n",
    "testPredict = model.predict(testX, batch_size=batch_size, verbose = 1)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "\n",
    "testPredict = scaler.inverse_transform(testPredict) #scaler2 \n",
    "testY = scaler.inverse_transform([testY]) #scaler2 \n",
    "\n",
    "# calculate root mean squared error \n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.5f RMSE' % (trainScore))  ## 1-RMSE = Accuracy %  \n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.5f RMSE' % (testScore))  ## 1-RMSE = Accuracy %  \n",
    "print('--------------------------------------------------------') \n",
    "print(' history.losses      = ', history.losses) # loss == mse \n",
    "print(' history.mses(=loss) = ', history.mses)  # loss == mse \n",
    "print(' history.maes        = ', history.maes)\n",
    "print(' history.mapes       = ', history.mapes)\n",
    "print(' history.val_losses     = ', history.val_losses)  # loss == mse \n",
    "print(' history.val_mses(=loss)= ', history.val_mses) # loss == mse \n",
    "print(' history.val_maes       = ', history.val_maes)\n",
    "print(' history.val_mapes      = ', history.val_mapes)\n",
    "print('--------------------------------------------------------')      \n",
    "# ---------------------------------\n",
    "# print to # Layer, Shape, Parameter \n",
    "print('model summary ... ', model.summary())  # keras.models.model()\n",
    "# ---------------------------------\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[timesteps:len(trainPredict)+timesteps, :] = trainPredict\n",
    "# shift test predictions for plotting ... NEW\n",
    "testPredictPlot = numpy.empty_like(dataset2)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "#testPredictPlot[len(trainPredict)+(timesteps*2)+1:len(dataset2)-1, :] = testPredict\n",
    "testPredictPlot[timesteps:len(testPredict)+timesteps, :] = testPredict\n",
    "# ---------------------------------\n",
    "plt.plot(scaler.inverse_transform(dataset), label='Groundtruth')\n",
    "plt.plot(trainPredictPlot, label='Training')\n",
    "plt.title('Train of Bearing3 X-axis from 12:17 to 14:07') \n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Samples')\n",
    "plt.legend() ## plt.plot(,label='')\n",
    "plt.savefig(title+'_Train_groundtruth.png') ###### 수정 \n",
    "plt.show()\n",
    "# ---------------------------------\n",
    "plt.plot(scaler2.inverse_transform(dataset2), label='Groundtruth')\n",
    "#plt.plot(trainPredictPlot) 불필요, 아래 test display\n",
    "plt.plot(testPredictPlot, label='Test')\n",
    "plt.title('Prediction Test of Bearing3 X-axis from 14:17 to 16:07') \n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Samples')\n",
    "plt.legend() ## plt.plot(,label='')\n",
    "plt.savefig(title+'_Test_groundtruth.png')  ###### 수정 \n",
    "plt.show()\n",
    "print('--------------------------------------------------------')  \n",
    "print(' history.losses(mse)     == ', history.losses) \n",
    "print(' history.val_losses(mse) == ', history.val_losses)\n",
    "print('--------------------------------------------------------') \n",
    "plt.plot(history.mses, 'b--', label='MSE of Train') # Train 0.000x소수점단위\n",
    "plt.plot(history.maes, 'b:', label='MAE of Train') # Train 0.000x소수점단위\n",
    "plt.plot(history.val_mses, 'ro', label='MSE of Test') # Test 0.000x 소수점단위\n",
    "plt.plot(history.val_maes, 'r+', label='MAE of Test') # Test 0.000x 소수점단위 \n",
    "plt.title('Train loss and Test loss of Bearing3 X-axis') \n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend() ## plt.plot(,label='')\n",
    "plt.savefig(title+'_mse_mae.png') ###### 수정 \n",
    "plt.show()\n",
    "print('--------------------------------------------------------')\n",
    "print('----------------------- The end of code ----------------') \n",
    "\"\"\"\n",
    "plt.plot(history.losses, 'bo')  # Train losses = MSE \n",
    "plt.plot(history.val_losses, 'b') # Test losses = MSE \n",
    "##### 아래 주의, x축은 epochs ### \n",
    "plt.axis([0, epochs, 0.00, 0.04]) ### y-end의 val_loss가 0.22를 넘지 않음.\n",
    "##### x-start(0), x-end(에폭 수), y-start(0), y-and #####\n",
    "plt.title('Model Loss, MAE of Bearing3 X-axis from 14:17 to 16:07')\n",
    "plt.ylabel('Loss(Mean Squared Error)')\n",
    "plt.xlabel('Epoch')\n",
    "#plt.legend(['Train', 'Test'], loc=0)  #plt.plot(,,,label=)\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.savefig(title+'_MAE_Model_Loss.png')\n",
    "#plt.figure() ## \n",
    "plt.show() \n",
    "print('--------------------------------------------------------')  \n",
    "plt.plot(history.mapes, 'bo') # Train MAPE\n",
    "plt.plot(history.val_mapes, 'b') # Test MAPE\n",
    "##### 아래 주의, x축은 epochs ### \n",
    "plt.axis([0, epochs, 0.00, 60000]) ### y-end의 val_loss가 30000를 넘지 않음.\n",
    "##### x-start(0), x-end(에폭 수), y-start(0), y-and #####\n",
    "plt.title('Prediction Loss(MAPE) of Bearing3 X-axis from 14:17 to 16:07')\n",
    "plt.ylabel('Loss(Mean Absolute Percentage Error)')\n",
    "plt.xlabel('Epoch')\n",
    "#plt.legend(['Train', 'Test'], loc=0)  #plt.plot(,,,label=)\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.savefig(title+'_MAPE_Predict_Loss.png')\n",
    "#plt.figure() ## \n",
    "plt.show() \n",
    "print('--------------------------------------------------------')  \n",
    "plt.plot(history.mses, label='MSE of Train') # Train 0.000x소수점단위\n",
    "plt.plot(history.maes, label='MAE of Train') # Train 0.000x소수점단위\n",
    "#plt.plot(history.mapes, label='mean_absolute_percentage_erro') # 1000~2000단위, 출력만 제외 \n",
    "##### 아래 주의, x축은 epochs ### \n",
    "plt.axis([0, epochs, 0.000, 0.040]) ### y-end의 val_loss가 0.025를 넘지 않음.\n",
    "##### x-start(0), x-end(에폭 수), y-start(0), y-and #####\n",
    "plt.title('Train Loss of Bearing3 X-axis from 12:17 to 14:07') \n",
    "plt.ylabel('Train Loss')\n",
    "plt.xlabel('Train Epoch')\n",
    "plt.legend() ## plt.plot(,label='')\n",
    "plt.savefig(title+'_Train_Loss.png') ###### 수정 \n",
    "plt.show()\n",
    "print('--------------------------------------------------------') \n",
    "plt.plot(history.val_mses, label='Test_MSE') # Test 0.000x 소수점단위\n",
    "plt.plot(history.val_maes, label='Test_MAE') # Test 0.000x 소수점단위 \n",
    "#plt.plot(history.val_mapes, label='validation_MAPE') # 1000~2000단위, 출력만 제외 \n",
    "##### 아래 주의, x축은 epochs ### \n",
    "plt.axis([0, epochs, 0.00, 0.28]) ### y-end의 val_loss가 0.14를 넘지 않음.\n",
    "##### x-start(0), x-end(에폭 수), y-start(0), y-and #####\n",
    "plt.title('Prediction Test Loss of Bearing3 X-axis from 14:17 to 16:07') \n",
    "plt.ylabel('Test Loss')\n",
    "plt.xlabel('Test Epoch')\n",
    "plt.legend() ## plt.plot(,label='')\n",
    "plt.savefig(title+'_Test_Loss.png') ###### 수정 \n",
    "plt.show()\n",
    "print('--------------------------------------------------------') \n",
    "# ---------------------------------\n",
    "series1 = dataframe #dataframe = pandas.read_csv(t, header=None, sep='\t', usecols=[4], engine='python')\n",
    "series1.plot(kind='kde') # ‘kde’ : Kernel Density Estimation plot, ‘line’ : line plot (default) \n",
    "plt.title('Density Plots of Train Dataset(Bearing3 X-axis from 12:17 to 14:07)') \n",
    "plt.savefig(title+'_dataset_DensityPlots.png') \n",
    "plt.show()\n",
    "# ---------------------------------\n",
    "series2 = dataframe2 #dataframe2 = pandas.read_csv(t1, header=None, sep='\t', usecols=[4], engine='python')\n",
    "series2.plot(kind='kde') # ‘kde’ : Kernel Density Estimation plot, ‘line’ : line plot (default) \n",
    "plt.title('Density Plots of Test Dataset(Bearing3 X-axis from 14:17 to 16:07)') \n",
    "plt.savefig(title+'_dataset2_DensityPlots.png') \n",
    "plt.show()\n",
    "# ---------------------------------\n",
    "# box and whisker plot\n",
    "# from pandas import DataFrame\n",
    "df_dataset = pandas.DataFrame(dataset) #Train dataset\n",
    "df_dataset.boxplot() \n",
    "plt.savefig(title+'_boxplot_dataset.png') \n",
    "plt.show() \n",
    "print('------- df_dataset.describe() -------')\n",
    "print(df_dataset.describe())\n",
    "# -------------------------------------\n",
    "df_trainPredictPlot = pandas.DataFrame(trainPredictPlot)\n",
    "df_trainPredictPlot.boxplot()  \n",
    "plt.savefig(title+'_boxplot_trainpredict.png') \n",
    "plt.show() \n",
    "print('------- df_trainPredictPlot.describe() -------')\n",
    "print(df_trainPredictPlot.describe())\n",
    "# -------------------------------------\n",
    "# box and whisker plot\n",
    "# from pandas import DataFrame\n",
    "df_dataset2 = pandas.DataFrame(dataset2) #Test dataset\n",
    "df_dataset2.boxplot() \n",
    "plt.savefig(title+'_boxplot_dataset2.png') \n",
    "plt.show() \n",
    "print('------- df_dataset2.describe() -------')\n",
    "print(df_dataset2.describe())\n",
    "# -------------------------------------\n",
    "df_testPredictPlot = pandas.DataFrame(testPredictPlot)\n",
    "df_testPredictPlot.boxplot()  # aa.boxplot()에서 aa는 dataframe 타입 \n",
    "plt.savefig(title+'_boxplot_test_predict.png') \n",
    "plt.show() \n",
    "print('------- df_testPredictPlot.describe() -------')\n",
    "print(df_testPredictPlot.describe())\n",
    "# -----------------------------------------------------\n",
    "\"\"\"\n",
    "# end of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
